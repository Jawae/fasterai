{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you sparsify your network, you don't really remove the parameters so you don't take advantage of it. There are a lot of research being done to accelerate computation on sparse matrices and we expect to have it implemented in PyTorch soon: [see `torch.sparse`](https://pytorch.org/docs/stable/sparse.html)\n",
    "\n",
    "What I mean here by pruning is the process of completely remove the sparsified parameters. This can thus only be done when the granularity is at the level of complete filters as it doesn't make sense to remove a single parameter.\n",
    "\n",
    " > Note: only Sequential feed-forward networks are supported for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMAGENETTE_160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (ImageList.from_folder(path)\n",
    "                .split_by_folder(train='train', valid='val')\n",
    "                .label_from_folder()\n",
    "                .transform(get_transforms(), size=128)\n",
    "                .databunch(bs=64)\n",
    "                .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasterai.sparsifier import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, mnist=True):\n",
    "        super().__init__()\n",
    "          \n",
    "        self.conv1 = nn.Conv2d(3, 12, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(12, 24, 5, 1)\n",
    "        self.conv3 = nn.Conv2d(24,36, 5, 1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1))\n",
    "        self.fc1 = nn.Linear(36, 18)\n",
    "        self.fc2 = nn.Linear(18, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, mnist=True):\n",
    "        super().__init__()\n",
    "          \n",
    "        self.conv1 = nn.Conv2d(3, 20, 5, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(20)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(50)\n",
    "        self.conv3 = nn.Conv2d(50, 100, 5, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(100)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1))\n",
    "        self.fc1 = nn.Linear(100, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, Net().cuda(), metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning of filter until a sparsity of 30%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.846674</td>\n",
       "      <td>1.647472</td>\n",
       "      <td>0.437707</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.510728</td>\n",
       "      <td>1.368574</td>\n",
       "      <td>0.556943</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.357512</td>\n",
       "      <td>1.366837</td>\n",
       "      <td>0.556178</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity at epoch 0: 7.59%\n",
      "Sparsity at epoch 1: 22.59%\n",
      "Sparsity at epoch 2: 30.00%\n",
      "Final Sparsity: 30.00\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(3, 1e-3, callbacks=[SparsifyCallback(learn, sparsity=30, granularity='filter', method='local', criteria='l1', sched_func=annealing_cos)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now our network has only half of its parameters that are used and still is able to achieve almost the same accuracy as when it was using 100% !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check that we correctly removed the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in Conv2d 1: 30.00%\n",
      "Sparsity in Conv2d 3: 30.00%\n",
      "Sparsity in Conv2d 5: 30.00%\n"
     ]
    }
   ],
   "source": [
    "for k,m in enumerate(learn.model.modules()):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        print(f\"Sparsity in {m.__class__.__name__} {k}: {100. * float(torch.sum(m.weight == 0))/ float(m.weight.nelement()):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasterai.pruner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner = Pruner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model = pruner.prune_model(learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_learn = Learner(data, pruned_model, metrics =[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original model had 55.62 % accuracy\n"
     ]
    }
   ],
   "source": [
    "print(f'The original model had {100*learn.validate()[1]:.2f} % accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pruned model has 57.25 % accuracy\n"
     ]
    }
   ],
   "source": [
    "print(f'The pruned model has {100*pruned_learn.validate()[1]:.2f} % accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprinsingly, the pruned model has even better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net\n",
       "======================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "======================================================================\n",
       "Conv2d               [20, 124, 124]       1,520      True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [20, 124, 124]       40         True      \n",
       "______________________________________________________________________\n",
       "Conv2d               [50, 58, 58]         25,050     True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [50, 58, 58]         100        True      \n",
       "______________________________________________________________________\n",
       "Conv2d               [100, 54, 54]        125,100    True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [100, 54, 54]        200        True      \n",
       "______________________________________________________________________\n",
       "AdaptiveAvgPool2d    [100, 1, 1]          0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [50]                 5,050      True      \n",
       "______________________________________________________________________\n",
       "Linear               [10]                 510        True      \n",
       "______________________________________________________________________\n",
       "\n",
       "Total params: 157,570\n",
       "Total trainable params: 157,570\n",
       "Total non-trainable params: 0\n",
       "Optimized with 'torch.optim.adam.Adam', betas=(0.9, 0.99)\n",
       "Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ \n",
       "Loss function : FlattenedLoss\n",
       "======================================================================\n",
       "Callbacks functions applied "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net\n",
       "======================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "======================================================================\n",
       "Conv2d               [14, 124, 124]       1,064      True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [14, 124, 124]       28         True      \n",
       "______________________________________________________________________\n",
       "Conv2d               [35, 58, 58]         12,285     True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [35, 58, 58]         70         True      \n",
       "______________________________________________________________________\n",
       "Conv2d               [70, 54, 54]         61,320     True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [70, 54, 54]         140        True      \n",
       "______________________________________________________________________\n",
       "AdaptiveAvgPool2d    [70, 1, 1]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [50]                 3,550      True      \n",
       "______________________________________________________________________\n",
       "Linear               [10]                 510        True      \n",
       "______________________________________________________________________\n",
       "\n",
       "Total params: 78,967\n",
       "Total trainable params: 78,967\n",
       "Total non-trainable params: 0\n",
       "Optimized with 'torch.optim.adam.Adam', betas=(0.9, 0.99)\n",
       "Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ \n",
       "Loss function : FlattenedLoss\n",
       "======================================================================\n",
       "Callbacks functions applied "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that our network has a lot of parameters removed, because we removed the filters that were not useful anymore (all of their weight were 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To-Do: generalize to other feed-forward models (e.g VGG16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
